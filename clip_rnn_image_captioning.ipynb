{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "clip_rnn_image_captioning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ66DKu72U_4"
      },
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_TkEgHjw00K"
      },
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image = preprocess(Image.open(\"clip.png\")).unsqueeze(0).to(device)\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image)\n",
        "    text_features = clip_model.encode_text(text)\n",
        "    \n",
        "    logits_per_image, logits_per_text = clip_model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4-tGI_Wdyw2"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc030jlBeNTX"
      },
      "source": [
        "!wget https://ivc.ischool.utexas.edu/VizWiz_final/images/train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaENe1znm0TD"
      },
      "source": [
        "!mkdir images\n",
        "!mv \"/content/train.zip\" \"/content/images/\"\n",
        "!unzip /content/images/train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiYJU_H8m87u"
      },
      "source": [
        "!mv \"/content/train\" \"/content/images/\"\n",
        "!rm /content/images/train.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1enIWVZn3Yw"
      },
      "source": [
        "!wget https://ivc.ischool.utexas.edu/VizWiz_final/images/val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwsnSGwroIEd"
      },
      "source": [
        "!mv \"/content/val.zip\" \"/content/images/\"\n",
        "!unzip /content/images/val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2BKyvBpoPV9"
      },
      "source": [
        "!mv \"/content/val\" \"/content/images/\"\n",
        "!rm /content/images/val.zip"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DNuL2oFoVSX"
      },
      "source": [
        "!wget https://ivc.ischool.utexas.edu/VizWiz_final/images/test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1q45xdToVpi"
      },
      "source": [
        "!mv \"/content/test.zip\" \"/content/images/\"\n",
        "!unzip /content/images/test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG535_vroV6L"
      },
      "source": [
        "!mv \"/content/test\" \"/content/images/\"\n",
        "!rm /content/images/test.zip"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFKCg4Vp0gmI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        ''' Initialize the layers of this model.'''\n",
        "        super().__init__()\n",
        "\n",
        "        # Keep track of hidden_size for initialization of hidden state\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer that turns words into a vector of a specified size\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # The LSTM takes embedded word vectors (of a specified size) as input\n",
        "        # and outputs hidden states of size hidden_dim\n",
        "        self.lstm = nn.LSTM(input_size=embed_size, \\\n",
        "                            hidden_size=hidden_size,  # LSTM hidden units\n",
        "                            num_layers=1,  # number of LSTM layer\n",
        "                            bias=True,  # use bias weights b_ih and b_hh\n",
        "                            batch_first=True,  # input & output will have batch size as 1st dimension\n",
        "                            dropout=0,  # Not applying dropout\n",
        "                            bidirectional=False,  # unidirectional LSTM\n",
        "                            )\n",
        "\n",
        "        # The linear layer that maps the hidden state output dimension\n",
        "        # to the number of words we want as output, vocab_size\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # initialize the hidden state\n",
        "        # self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\" At the start of training, we need to initialize a hidden state;\n",
        "        there will be none because the hidden state is formed based on previously seen data.\n",
        "        So, this function defines a hidden state with all zeroes\n",
        "        The axes semantics are (num_layers, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n",
        "                torch.zeros((1, batch_size, self.hidden_size), device=device))\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \"\"\" Define the feedforward behavior of the model \"\"\"\n",
        "\n",
        "        # Discard the <end> word to avoid predicting when <end> is the input of the RNN\n",
        "        captions = captions[:, :-1]\n",
        "\n",
        "        # Initialize the hidden state\n",
        "        self.batch_size = features.shape[0]  # features is of shape (batch_size, embed_size)\n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "        \n",
        "        # Create embedded word vectors for each word in the captions\n",
        "        embeddings = self.word_embeddings(\n",
        "            captions)  # embeddings new shape : (batch_size, captions length - 1, embed_size)\n",
        "\n",
        "        # Stack the features and captions\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings),\n",
        "                               dim=1)  # embeddings new shape : (batch_size, caption length, embed_size)\n",
        "\n",
        "        # Get the output and hidden state by passing the lstm over our word embeddings\n",
        "        # the lstm takes in our embeddings and hidden state\n",
        "        lstm_out, self.hidden = self.lstm(embeddings,\n",
        "                                          self.hidden)  # lstm_out shape : (batch_size, caption length, hidden_size)\n",
        "\n",
        "        # Fully connected layer\n",
        "        outputs = self.linear(lstm_out)  # outputs shape : (batch_size, caption length, vocab_size)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    ## Greedy search\n",
        "    def sample(self, inputs):\n",
        "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
        "\n",
        "        output = []\n",
        "        batch_size = inputs.shape[0]  # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)\n",
        "        hidden = self.init_hidden(batch_size)  # Get initial hidden state of the LSTM\n",
        "\n",
        "        while True:\n",
        "            lstm_out, hidden = self.lstm(inputs, hidden)  # lstm_out shape : (1, 1, hidden_size)\n",
        "            outputs = self.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
        "            outputs = outputs.squeeze(1)  # outputs shape : (1, vocab_size)\n",
        "            _, max_indice = torch.max(outputs, dim=1)  # predict the most likely next word, max_indice shape : (1)\n",
        "\n",
        "            output.append(max_indice.cpu().numpy()[0].item())  # storing the word predicted\n",
        "\n",
        "            if (max_indice == 1):\n",
        "                # We predicted the <end> word, so there is no further prediction to do\n",
        "                break\n",
        "\n",
        "            ## Prepare to embed the last predicted word to be the new input of the lstm\n",
        "            inputs = self.word_embeddings(max_indice)  # inputs shape : (1, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)  # inputs shape : (1, 1, embed_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "    ## Beam search implementation (Attempt)\n",
        "    def beam_search_sample(self, inputs, beam=3):\n",
        "        output = []\n",
        "        batch_size = inputs.shape[0]  # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)\n",
        "        hidden = self.init_hidden(batch_size)  # Get initial hidden state of the LSTM\n",
        "\n",
        "        # sequences[0][0] : index of start word\n",
        "        # sequences[0][1] : probability of the word predicted\n",
        "        # sequences[0][2] : hidden state related of the last word\n",
        "        sequences = [[[torch.Tensor([0])], 1.0, hidden]]\n",
        "        max_len = 20\n",
        "\n",
        "        ## Step 1\n",
        "        # Predict the first word <start>\n",
        "        outputs, hidden = DecoderRNN.get_outputs(self, inputs, hidden)\n",
        "        _, max_indice = torch.max(outputs, dim=1)  # predict the most likely next word, max_indice shape : (1)\n",
        "        output.append(max_indice.cpu().numpy()[0].item())  # storing the word predicted\n",
        "        # inputs = DecoderRNN.get_next_word_input(self, max_indice)\n",
        "\n",
        "        l = 0\n",
        "        while len(sequences[0][0]) < max_len:\n",
        "            print(\"l:\", l)\n",
        "            l += 1\n",
        "            temp = []\n",
        "            for seq in sequences:\n",
        "                #                 print(\"seq[0]: \", seq[0])\n",
        "                inputs = seq[0][-1]  # last word index in seq\n",
        "                inputs = inputs.type(torch.cuda.LongTensor)\n",
        "                print(\"inputs : \", inputs)\n",
        "                # Embed the input word\n",
        "                inputs = self.word_embeddings(inputs)  # inputs shape : (1, embed_size)\n",
        "                inputs = inputs.unsqueeze(1)  # inputs shape : (1, 1, embed_size)\n",
        "\n",
        "                # retrieve the hidden state\n",
        "                hidden = seq[2]\n",
        "\n",
        "                preds, hidden = DecoderRNN.get_outputs(self, inputs, hidden)\n",
        "\n",
        "                # Getting the top <beam_index>(n) predictions\n",
        "                softmax_score = F.log_softmax(outputs, dim=1)  # Define a function to sort the cumulative score\n",
        "                sorted_score, indices = torch.sort(-softmax_score, dim=1)\n",
        "                word_preds = indices[0][:beam]\n",
        "                best_scores = sorted_score[0][:beam]\n",
        "\n",
        "                # Creating a new list so as to put them via the model again\n",
        "                for i, w in enumerate(word_preds):\n",
        "                    #                     print(\"seq[0]: \", seq[0][0][:].cpu().numpy().item())\n",
        "                    next_cap, prob = seq[0][0].cpu().numpy().tolist(), seq[1]\n",
        "\n",
        "                    next_cap.append(w)\n",
        "                    print(\"next_cap : \", next_cap)\n",
        "                    prob * best_scores[i].cpu().item()\n",
        "                    temp.append([next_cap, prob])\n",
        "\n",
        "            sequences = temp\n",
        "            # Order according to proba\n",
        "            ordered = sorted(sequences, key=lambda tup: tup[1])\n",
        "\n",
        "            # Getting the top words\n",
        "            sequences = ordered[:beam]\n",
        "            print(\"sequences: \", sequences)\n",
        "\n",
        "    def get_outputs(self, inputs, hidden):\n",
        "        lstm_out, hidden = self.lstm(inputs, hidden)  # lstm_out shape : (1, 1, hidden_size)\n",
        "        outputs = self.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
        "        outputs = outputs.squeeze(1)  # outputs shape : (1, vocab_size)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "    def get_next_word_input(self, max_indice):\n",
        "        ## Prepare to embed the last predicted word to be the new input of the lstm\n",
        "        inputs = self.word_embeddings(max_indice)  # inputs shape : (1, embed_size)\n",
        "        inputs = inputs.unsqueeze(1)  # inputs shape : (1, 1, embed_size)\n",
        "\n",
        "        return inputs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZT3kYrF8lh7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from data_loader import get_loader\n",
        "from torchvision import transforms\n",
        "#Malikeh: from models import EncoderCNN, DecoderRNN\n",
        "import math\n",
        "import utils\n",
        "import json\n",
        "\n",
        "def validate(clip_model, decoder, criterion, data_loader, vocab_size, epoch, device='cpu', save_captions=False, linear_layer = None):\n",
        "    with torch.no_grad():\n",
        "        #Malikeh: encoder.eval()\n",
        "        if linear_layer != None:\n",
        "          linear_layer.eval()\n",
        "        decoder.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        total_step = len(data_loader.dataset.paths) #number of images in val dataset\n",
        "        predicted_captions = []\n",
        "        print(\"Running Validation...\")\n",
        "        for batch in data_loader:\n",
        "            # Obtain the batch.\n",
        "            images, captions, img_id = batch #next(iter(data_loader))\n",
        "            # Move batch of images and captions to GPU if CUDA is available.\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Pass the inputs through the CNN-RNN model.\n",
        "            features = clip_model.encode_image(images)\n",
        "            if linear_layer != None:\n",
        "                features = linear_layer(features)\n",
        "            outputs = decoder(features, captions)\n",
        "\n",
        "            # Calculate the batch loss.\n",
        "            loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
        "            val_loss += loss\n",
        "\n",
        "            if save_captions:\n",
        "                pred = decoder.sample(features.unsqueeze(1))\n",
        "                caption = utils.clean_sentence(pred, data_loader)\n",
        "                predicted_captions.append({\"image_id\": int(img_id), \"caption\": str(caption)})\n",
        "\n",
        "        if save_captions:\n",
        "            with open('val_captions.json', 'w') as fp:\n",
        "                json.dump(predicted_captions, fp)\n",
        "\n",
        "        val_loss /= total_step\n",
        "        print(\"Validation Loss for epoch \" + str(epoch) + ': ' + str(float(val_loss)))\n",
        "        return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea99-ggxw4Tr"
      },
      "source": [
        "###Clip Without Linear Layer\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "from data_loader import get_loader\n",
        "import math\n",
        "import ssl\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "#Malikeh:from models import EncoderCNN, DecoderRNN\n",
        "import utils\n",
        "#Malikeh:import validation\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    batch_size = 32            # batch size\n",
        "    vocab_threshold = 6        # minimum word count threshold\n",
        "    vocab_from_file = False    # if True, load existing vocab file\n",
        "    embed_size = 512           # dimensionality of image and word embeddings\n",
        "    hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
        "    num_epochs = 1             # number of training epochs (1 for testing)\n",
        "    save_every = 1             # determines frequency of saving model weights\n",
        "    print_every = 200          # determines window for printing average loss\n",
        "    log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "        transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "        transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "        transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # Build data loader.\n",
        "    data_loader = get_loader(transform=transform_train, mode='train', batch_size=batch_size, vocab_threshold=vocab_threshold,\n",
        "                             vocab_from_file=vocab_from_file)\n",
        "    val_data_loader = get_loader(transform=transform_train, mode='val')\n",
        "\n",
        "    # The size of the vocabulary.\n",
        "    vocab_size = len(data_loader.dataset.vocab)\n",
        "    print(vocab_size)\n",
        "\n",
        "    # Initialize the encoder and decoder.\n",
        "    #Malikeh:encoder = EncoderCNN(embed_size)\n",
        "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "    # Move models to GPU if CUDA is available.\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    #Malikeh:encoder.to(device)\n",
        "    decoder.to(device)\n",
        "\n",
        "    # Define the loss function.\n",
        "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "\n",
        "    params = list(decoder.parameters()) #Malikeh + list(encoder.embed.parameters())\n",
        "\n",
        "    optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "    # optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
        "    #optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)\n",
        "\n",
        "    # Set the total number of training steps per epoch.\n",
        "    total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
        "    #total_step = 1\n",
        "\n",
        "    # Open the training log file.\n",
        "    f = open(log_file, 'w')\n",
        "\n",
        "    # Collect losses in these arrays\n",
        "    training_loss_per_epoch = []\n",
        "    val_loss_per_epoch = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        avg_batch_loss = 0\n",
        "        avg_training_loss = 0\n",
        "\n",
        "        #Malikeh:encoder.train()\n",
        "        decoder.train()\n",
        "\n",
        "        for i_step in range(1, total_step + 1):\n",
        "\n",
        "            # Randomly sample a caption length, and sample indices with that length.\n",
        "            indices = data_loader.dataset.get_train_indices()\n",
        "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "            data_loader.batch_sampler.sampler = new_sampler\n",
        "\n",
        "            # Obtain the batch.\n",
        "            images, captions = next(iter(data_loader))\n",
        "\n",
        "            # Move batch of images and captions to GPU if CUDA is available.\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Zero the gradients.\n",
        "            decoder.zero_grad()\n",
        "            #Malikeh:encoder.zero_grad()\n",
        "\n",
        "            # Pass the inputs through the CNN-RNN model.\n",
        "            #Malikeh: features = encoder(images)\n",
        "            with torch.no_grad():\n",
        "              features = clip_model.encode_image(images)\n",
        "            outputs = decoder(features, captions)\n",
        "\n",
        "            # Calculate the batch loss.\n",
        "            #         print(\"outputs.shape: \", outputs.shape)\n",
        "            loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
        "            avg_batch_loss += loss\n",
        "            avg_training_loss += loss\n",
        "\n",
        "            # Backward pass.\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters in the optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Get training statistics.\n",
        "            stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (\n",
        "            epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
        "\n",
        "            # Print training statistics (on same line).\n",
        "            print('\\r' + stats, end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # Print training statistics to file.\n",
        "            f.write(stats + '\\n')\n",
        "            f.flush()\n",
        "\n",
        "            # Print training statistics (on different line).\n",
        "            if i_step % print_every == 0:\n",
        "                print('\\r' + stats)\n",
        "\n",
        "        # Save the weights.\n",
        "        if epoch % save_every == 0:\n",
        "            torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
        "            #Malikeh:torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
        "\n",
        "        avg_batch_loss /= total_step\n",
        "        training_loss_per_epoch.append(avg_batch_loss)\n",
        "\n",
        "        \n",
        "        print(\"\\nTraining Loss for epoch \" + str(epoch) + ': ' + str(float(avg_batch_loss)))\n",
        "        val_loss = validate(clip_model, decoder, criterion, val_data_loader, vocab_size, epoch, device = device, save_captions=False, linear_layer = None)\n",
        "        val_loss_per_epoch.append(val_loss)\n",
        "\n",
        "\n",
        "    # Close the training log file.\n",
        "    f.close()\n",
        "\n",
        "    utils.plotLosses(training_loss_per_epoch,\n",
        "                     val_loss_per_epoch,\n",
        "                     'Cross Entropy Loss (per Epoch)')\n",
        "\n",
        "\n",
        "    # test_data_loader = get_loader(transform=transform_train, mode='test')\n",
        "    # test(clip_model, decoder, test_data_loader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymYX8aFw-Tky"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out.cuda().float())  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden.float())  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1)).float()).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=768, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out.float())  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out.float())\n",
        "        return h, c  \n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            \n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWIt8Chru7W-"
      },
      "source": [
        "def get_clip_features(x):\n",
        "  x1 = clip_model.visual.conv1(x.half()) #torch.Size([1, 768, 7, 7])\n",
        "  x2 = x1.reshape(x1.shape[0], x1.shape[1], -1) #torch.Size([1, 768, 49])\n",
        "  x3 = x2.permute(0, 2, 1) #torch.Size([1, 768, 49])\n",
        "  x4 = torch.cat([clip_model.visual.class_embedding.to(x3.dtype) + torch.zeros(x3.shape[0], 1, x3.shape[-1], dtype=x3.dtype, device=x3.device), x3], dim=1)\n",
        "  #torch.Size([1, 50, 768])\n",
        "  x5 = x4 + clip_model.visual.positional_embedding.to(x4.dtype) #torch.Size([1, 50, 768])\n",
        "  x6 = clip_model.visual.ln_pre(x5) #torch.Size([1, 50, 768])\n",
        "  x7 = x6.permute(1, 0, 2)  # NLD -> LND #torch.Size([50, 1, 768])\n",
        "  x8 = clip_model.visual.transformer(x7) #torch.Size([50, 1, 768])\n",
        "  x9 = x8.permute(1, 0, 2)  # LND -> NLD torch.Size([1, 50, 768])\n",
        "  return x9"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r3WQdYQfxSM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from data_loader import get_loader\n",
        "from torchvision import transforms\n",
        "#Malikeh: from models import EncoderCNN, DecoderRNN\n",
        "import math\n",
        "import utils\n",
        "import json\n",
        "\n",
        "def validate(clip_model, decoder, criterion, data_loader, vocab_size, epoch, device='cpu', save_captions=False, linear_layer = None):\n",
        "    with torch.no_grad():\n",
        "        #Malikeh: encoder.eval()\n",
        "        if linear_layer != None:\n",
        "          linear_layer.eval()\n",
        "        decoder.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        total_step = len(data_loader.dataset.paths) #number of images in val dataset\n",
        "        predicted_captions = []\n",
        "        print(\"Running Validation...\")\n",
        "        for batch in data_loader:\n",
        "            # Obtain the batch.\n",
        "            images, captions, img_id = batch #next(iter(data_loader))\n",
        "            # Move batch of images and captions to GPU if CUDA is available.\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Pass the inputs through the CNN-RNN model.\n",
        "            features = get_clip_features(images)\n",
        "            if linear_layer != None:\n",
        "                features = linear_layer(features)\n",
        "            #outputs = decoder(features, captions)\n",
        "            caplens = torch.tensor([captions.shape[1]]).reshape(1,1)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, captions, caplens)\n",
        "\n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "            scores = torch.nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets = torch.nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "\n",
        "            # Calculate the batch loss.\n",
        "            loss = criterion(scores.data, targets.data)\n",
        "            #loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
        "            val_loss += loss\n",
        "\n",
        "        val_loss /= total_step\n",
        "        print(\"Validation Loss for epoch \" + str(epoch) + ': ' + str(float(val_loss)))\n",
        "        return val_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlcwyqIYeJ7Z",
        "outputId": "4a555f91-8dc3-43f6-fbbe-fc20a512c8f8"
      },
      "source": [
        "###Clip With Attention\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "from data_loader import get_loader\n",
        "import math\n",
        "import ssl\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "#Malikeh:from models import EncoderCNN, DecoderRNN\n",
        "import utils\n",
        "#Malikeh:import validation\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "                \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    batch_size = 32            # batch size\n",
        "    vocab_threshold = 6        # minimum word count threshold\n",
        "    vocab_from_file = False    # if True, load existing vocab file\n",
        "    embed_size = 512           # dimensionality of image and word embeddings\n",
        "    hidden_size = 512          # number of image_features in hidden state of the RNN decoder\n",
        "    num_epochs = 5             # number of training epochs (1 for testing)\n",
        "    save_every = 1             # determines frequency of saving model weights\n",
        "    print_every = 200          # determines window for printing average loss\n",
        "    log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
        "\n",
        "    attention_dim = 512\n",
        "    dropout = 0.5\n",
        "    alpha_c = 1.\n",
        "    encoder_dim=512\n",
        "    grad_clip = 5.\n",
        "    fine_tune_encoder = True\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "        transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "        transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "        transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # Build data loader.\n",
        "    data_loader = get_loader(transform=transform_train, mode='train', batch_size=batch_size, vocab_threshold=vocab_threshold,\n",
        "                             vocab_from_file=vocab_from_file)\n",
        "    val_data_loader = get_loader(transform=transform_train, mode='val')\n",
        "\n",
        "    # The size of the vocabulary.\n",
        "    vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "    # Initialize the encoder and decoder.\n",
        "    # encoder = Encoder() #att-enc\n",
        "    # encoder.fine_tune(fine_tune_encoder)\n",
        "    #Malikeh: decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "    decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                   embed_dim=embed_size,\n",
        "                                   decoder_dim=hidden_size,\n",
        "                                   vocab_size=vocab_size,\n",
        "                                   dropout=dropout)\n",
        "\n",
        "    # Move models to GPU if CUDA is available.\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    #encoder.to(device) #att-enc\n",
        "    decoder.to(device)\n",
        "\n",
        "    # Define the loss function.\n",
        "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "\n",
        "    params = list(decoder.parameters()) #+ list(encoder.parameters()) #att-enc\n",
        "\n",
        "    #Malikeh: optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "    optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "    # optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
        "    # optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)\n",
        "\n",
        "    # Set the total number of training steps per epoch.\n",
        "    total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
        "    #total_step = 1\n",
        "\n",
        "    # Open the training log file.\n",
        "    f = open(log_file, 'w')\n",
        "\n",
        "    # Collect losses in these arrays\n",
        "    training_loss_per_epoch = []\n",
        "    val_loss_per_epoch = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        avg_batch_loss = 0\n",
        "\n",
        "        #encoder.train() #att-enc\n",
        "        decoder.train()\n",
        "\n",
        "        for i_step in range(1, total_step + 1):\n",
        "\n",
        "            # Randomly sample a caption length, and sample indices with that length.\n",
        "            indices = data_loader.dataset.get_train_indices()\n",
        "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "            data_loader.batch_sampler.sampler = new_sampler\n",
        "\n",
        "            # Obtain the batch.\n",
        "            images, captions = next(iter(data_loader))\n",
        "            \n",
        "\n",
        "            # Move batch of images and captions to GPU if CUDA is available.\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Zero the gradients.\n",
        "            decoder.zero_grad()\n",
        "            # encoder.zero_grad()\n",
        "\n",
        "            # Pass the inputs through the CNN-RNN model.\n",
        "            # image_features = encoder(images)\n",
        "            with torch.no_grad():\n",
        "              image_features = get_clip_features(images)\n",
        "              #image_features = clip_model.encode_image(images)\n",
        "            #Malikeh: outputs = decoder(image_features, captions)\n",
        "            caplens = [data_loader.dataset.caption_lengths[index] for index in indices] \n",
        "            caplens = torch.tensor(caplens).reshape(len(indices),1)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(image_features, captions, caplens)\n",
        "\n",
        "\n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "            scores = torch.nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets = torch.nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Calculate the batch loss.\n",
        "            #         print(\"outputs.shape: \", outputs.shape)\n",
        " \n",
        "            loss = criterion(scores.data, targets.data)\n",
        "            \n",
        "            avg_batch_loss += loss\n",
        "\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Backward pass.\n",
        "            loss.backward()\n",
        "\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "            \n",
        "\n",
        "            # Update the parameters in the optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Get training statistics.\n",
        "            stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (\n",
        "            epoch, num_epochs, i_step, total_step, loss.item())\n",
        "\n",
        "            # Print training statistics (on same line).\n",
        "            print('\\r' + stats, end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # Print training statistics to file.\n",
        "            f.write(stats + '\\n')\n",
        "            f.flush()\n",
        "\n",
        "            # Print training statistics (on different line).\n",
        "            if i_step % print_every == 0:\n",
        "                print('\\r' + stats)\n",
        "            \n",
        "\n",
        "        # Save the weights.\n",
        "        if epoch % save_every == 0:\n",
        "            torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
        "            #Malikeh:torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
        "\n",
        "        avg_batch_loss /= total_step\n",
        "        training_loss_per_epoch.append(avg_batch_loss)\n",
        "\n",
        "        \n",
        "        print(\"\\nTraining Loss for epoch \" + str(epoch) + ': ' + str(float(avg_batch_loss)))\n",
        "        val_loss = validate(clip_model, decoder, criterion, val_data_loader, vocab_size, epoch, device = device, save_captions=False, linear_layer = None)\n",
        "        val_loss_per_epoch.append(val_loss)\n",
        "\n",
        "\n",
        "    # Close the training log file.\n",
        "    f.close()\n",
        "\n",
        "    utils.plotLosses(training_loss_per_epoch,\n",
        "                     val_loss_per_epoch,\n",
        "                     'Cross Entropy Loss (per Epoch)')\n",
        "\n",
        "\n",
        "    # test_data_loader = get_loader(transform=transform_train, mode='test')\n",
        "    # test(clip_model, decoder, test_data_loader, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading annotations into memory...\n",
            "Done (t=0.64s)\n",
            "creating index...\n",
            "index created! imgs = 23431, anns = 100575\n",
            "[0/100575] Tokenizing captions...\n",
            "[100000/100575] Tokenizing captions...\n",
            "loading annotations into memory...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100575 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done (t=0.24s)\n",
            "creating index...\n",
            "index created! imgs = 23431, anns = 100575\n",
            "Obtaining caption lengths...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 100575/100575 [00:10<00:00, 9354.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary successfully loaded from vocab.pkl file!\n",
            "loading annotations into memory...\n",
            "Done (t=0.20s)\n",
            "creating index...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|         | 934/33145 [00:00<00:03, 9338.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "index created! imgs = 7750, anns = 33145\n",
            "Obtaining caption lengths...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 33145/33145 [00:03<00:00, 9450.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [200/3143], Loss: 4.7409\n",
            "Epoch [1/5], Step [400/3143], Loss: 4.8517\n",
            "Epoch [1/5], Step [600/3143], Loss: 4.7153\n",
            "Epoch [1/5], Step [800/3143], Loss: 4.3040\n",
            "Epoch [1/5], Step [804/3143], Loss: 4.5668"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1_lxW_9l7wc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}