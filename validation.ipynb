{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "validation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2iouBXq4xSh",
        "outputId": "1264c6ef-aecd-42c1-fa42-29b4284f418b"
      },
      "source": [
        "!unzip Models.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Models.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of Models.zip or\n",
            "        Models.zip.zip, and cannot find Models.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLYNMeZ25PC_"
      },
      "source": [
        "# test validation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                          (0.229, 0.224, 0.225))])\n",
        "\n",
        "batch_size = 32            # batch size\n",
        "vocab_threshold = 6        # minimum word count threshold\n",
        "vocab_from_file = False    # if True, load existing vocab file\n",
        "embed_size = 512           # dimensionality of image and word embeddings\n",
        "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
        "\n",
        "val_data_loader = get_loader(transform=transform_train, mode='val')\n",
        "\n",
        "vocab_size = len(val_data_loader.dataset.vocab)\n",
        "\n",
        "# Initialize the encoder and decoder.\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Load the trained weights.\n",
        "MODEL_DIR = './Models/BatchSizes/1epoch_' + str(batch_size) + 'batch/'\n",
        "encoder_path = os.path.join(MODEL_DIR, 'encoder-1epoch-%dbatch.pkl' % batch_size)\n",
        "decoder_path = os.path.join(MODEL_DIR, 'decoder-1epoch-%dbatch.pkl' % batch_size)\n",
        "encoder.load_state_dict(torch.load(encoder_path, map_location=torch.device('cpu')))\n",
        "decoder.load_state_dict(torch.load(decoder_path, map_location=torch.device('cpu')))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "\n",
        "# Define the loss function.\n",
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "_, predicted_captions = validate(encoder, decoder, criterion, val_data_loader, vocab_size, 1, device, save_captions=True)\n",
        "\n",
        "\n",
        "RESULTS_DIR = './results/BatchSizes/1epoch_' + str(batch_size) + 'batch/''\n",
        "\n",
        "with open(os.path.join(RESULTS_DIR, 'val_captions.json'), 'w')) as fp:\n",
        "    json.dump(predicted_captions, fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}