{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hkDMYqQLocs"
      },
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dx2suMb71L1"
      },
      "source": [
        "#download classification data\n",
        "!curl -L \"https://public.roboflow.com/ds/pmXBn8TqW8?key=ZZm6kgr3sf\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddLZY4Mm79ge"
      },
      "source": [
        "import os\n",
        "#our the classes and images we want to test are stored in folders in the test set\n",
        "train_class_names = os.listdir('./test/')\n",
        "train_class_names.remove('_tokenization.txt')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqWkg1wH8AJK",
        "outputId": "858b6af2-5bd5-499a-a20f-5503571917d6"
      },
      "source": [
        "#we auto generate some example tokenizations in Roboflow but you should edit this file to try out your own prompts\n",
        "#CLIP gets a lot better with the right prompting!\n",
        "#be sure the tokenizations are in the same order as your class_names above!\n",
        "%cat ./test/_tokenization.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An example picture from the Flowers_Classification dataset depicting a daisy\n",
            "An example picture from the Flowers_Classification dataset depicting a dandelion"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZnIt5e38CAB"
      },
      "source": [
        "#edit your prompts as you see fit here\n",
        "# %%writefile ./test/_tokenization.txt\n",
        "# An example picture from the flowers dataset depicting a daisy\n",
        "# An example picture from the flowers dataset depicting a dandelion"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNlnj1Nd8FbD"
      },
      "source": [
        "train_candidate_captions = []\n",
        "with open('./test/_tokenization.txt') as f:\n",
        "    train_candidate_captions = f.read().splitlines()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fiYriD_8HQm"
      },
      "source": [
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7UzFmgd8Rd5"
      },
      "source": [
        "import clip\n",
        "import torch\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
        "model, transform = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTPIkshS-KH0"
      },
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class image_caption_dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.images = df[\"image\"].tolist()\n",
        "        self.caption = df[\"caption\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        images = transform(Image.open(self.images[idx])) #preprocess from clip.load\n",
        "        caption = self.caption[idx]\n",
        "        return images,caption"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3giIq5QANvu"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "imageList = []\n",
        "captionList = []\n",
        "\n",
        "for i, cls in enumerate(train_class_names):\n",
        "    train_imgs = glob.glob('./train/' + cls + '/*.jpg')\n",
        "    for img in train_imgs:\n",
        "        imageList.append(img)\n",
        "        captionList.append(train_candidate_captions[i])\n",
        "\n",
        "listOfTuples = list(zip(imageList, captionList)) \n",
        "  \n",
        "# Converting lists of tuples into pandas Dataframe. \n",
        "df = pd.DataFrame(listOfTuples,\n",
        "                  columns = ['image', 'caption'])\n",
        "\n",
        "dataset = image_caption_dataset(df)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo77lnF-EeFh"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE, shuffle = True) #Define your own dataloader"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us-KIKv4KUq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b501de-7bdf-42a0-d857-f38d89239030"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model.float()\n",
        "else :\n",
        "    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params from paper\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
        "\n",
        "        # images= torch.stack([preprocess(Image.fromarray(img)) for img in list_image],dim=0) # omit the Image.fromarray if the images already in PIL format, change this line to images=list_image if using preprocess inside the dataset class\n",
        "        images = list_image.to(device)\n",
        "        texts = clip.tokenize(list_txt).to(device)\n",
        "\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "        if device == \"cpu\":\n",
        "            ground_truth = torch.arange(list_image.shape[0]).half().to(device) # Did not use BATCH_SIZE as last batch might be incomplete and cause error\n",
        "        else:\n",
        "            ground_truth = torch.arange(list_image.shape[0]).long().to(device) # Did not use BATCH_SIZE as last batch might be incomplete and cause error\n",
        "\n",
        "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "        total_loss.backward()\n",
        "        \n",
        "        if device == \"cpu\":\n",
        "            optimizer.step()\n",
        "        else :\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            clip.model.convert_weights(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkvxbQnNMoqI"
      },
      "source": [
        "# torch.save(model, \"trainedModel.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT-jegK444lv"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download('/content/trainedModel.pt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdh3rtAaAK4d"
      },
      "source": [
        "# Testing\n",
        "test_class_names = os.listdir('./test/')\n",
        "test_class_names.remove('_tokenization.txt')\n",
        "\n",
        "#edit your prompts as you see fit here\n",
        "# %%writefile ./test/_tokenization.txt\n",
        "# An example picture from the flowers dataset depicting a daisy\n",
        "# An example picture from the flowers dataset depicting a dandelion\n",
        "\n",
        "test_candidate_captions = []\n",
        "with open('./test/_tokenization.txt') as f:\n",
        "    test_candidate_captions = f.read().splitlines()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H9aDpYAGLas",
        "outputId": "0b0e5da5-6559-4e17-be6f-48b61187a194"
      },
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def argmax(iterable):\n",
        "    return max(enumerate(iterable), key=lambda x: x[1])[0]\n",
        "\n",
        "correct = []\n",
        "\n",
        "#define our target classificaitons, you can should experiment with these strings of text as you see fit, though, make sure they are in the same order as your class names above\n",
        "text = clip.tokenize(test_candidate_captions).to(device)\n",
        "\n",
        "for cls in test_class_names:\n",
        "    class_correct = []\n",
        "    test_imgs = glob.glob('./test/' + cls + '/*.jpg')\n",
        "    for img in test_imgs:\n",
        "        #print(img)\n",
        "        image = transform(Image.open(img)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image)\n",
        "            text_features = model.encode_text(text)\n",
        "            \n",
        "            logits_per_image, logits_per_text = model(image, text)\n",
        "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "            pred = test_class_names[argmax(list(probs)[0])]\n",
        "            #print(pred)\n",
        "            if pred == cls:\n",
        "                correct.append(1)\n",
        "                class_correct.append(1)\n",
        "            else:\n",
        "                correct.append(0)\n",
        "                class_correct.append(0)\n",
        "    \n",
        "    print('accuracy on class ' + cls + ' is :' + str(sum(class_correct)/len(class_correct)))\n",
        "print('accuracy on all is : ' + str(sum(correct)/len(correct)))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on class dandelion is :0.6095238095238096\n",
            "accuracy on class daisy is :0.7792207792207793\n",
            "accuracy on all is : 0.6813186813186813\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}